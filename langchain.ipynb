{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain\n",
    "\n",
    "zhipuai_api_key = \"41e34b6365edaa50266d9ed13a1f953b.CodlLOLDqAj7UvEK\"\n",
    "\n",
    "!pip install zhipuai\n",
    "\n",
    "from zhipuai import ZhipuAI\n",
    "client = ZhipuAI(api_key=zhipuai_api_key) # 填写您自己的APIKey\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"你好\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"我是人工智能助手\"},\n",
    "        {\"role\": \"user\", \"content\": \"你叫什么名字\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"我叫chatGLM\"},\n",
    "        {\"role\": \"user\", \"content\": \"你都可以做些什么事\"}\n",
    "        \n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message)\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "\n",
    "import json\n",
    "from langchain.llms.base import LLM\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from typing import List, Optional\n",
    "from zhipuai import ZhipuAI\n",
    "from langchain_core.messages.ai import AIMessage\n",
    "zhipuai_api_key = \"1354616636358b562965c51dc1da555d.sBYNC7rIcp21aEoe\"\n",
    "\n",
    "def tool_config_from_file(tool_name, directory=\"Tool/\"):\n",
    "    \"\"\"search tool yaml and return json format\"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('.yaml') and tool_name in filename:\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, encoding='utf-8') as f:\n",
    "                return yaml.safe_load(f)\n",
    "    return None\n",
    "\n",
    "\n",
    "class ChatGLM4(LLM):\n",
    "    max_token: int = 8192\n",
    "    do_sample: bool = True\n",
    "    temperature: float = 0.8\n",
    "    top_p = 0.8\n",
    "    tokenizer: object = None\n",
    "    model: object = None\n",
    "    history: List = []\n",
    "    tool_names: List = []\n",
    "    has_search: bool = False\n",
    "    client:object =None\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.client = ZhipuAI(api_key=zhipuai_api_key) \n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"ChatGLM4\"\n",
    "\n",
    "    \n",
    "    def stream(self,prompt:str,history=[]):\n",
    "        if history is None:\n",
    "            history=[]\n",
    "            \n",
    "        history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "            messages=history,\n",
    "            stream=True,\n",
    "        )\n",
    "        for chunk in response:\n",
    "            yield chunk.choices[0].delta.content\n",
    "        \n",
    "        \n",
    "\n",
    "    def _tool_history(self, prompt: str):\n",
    "        ans = []\n",
    "        tool_prompts = prompt.split(\n",
    "            \"You have access to the following tools:\\n\\n\")[1].split(\"\\n\\nUse a json blob\")[0].split(\"\\n\")\n",
    "\n",
    "        tool_names = [tool.split(\":\")[0] for tool in tool_prompts]\n",
    "        self.tool_names = tool_names\n",
    "        tools_json = []\n",
    "        for i, tool in enumerate(tool_names):\n",
    "            tool_config = tool_config_from_file(tool)\n",
    "            if tool_config:\n",
    "                tools_json.append(tool_config)\n",
    "            else:\n",
    "                ValueError(\n",
    "                    f\"Tool {tool} config not found! It's description is {tool_prompts[i]}\"\n",
    "                )\n",
    "\n",
    "        ans.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Answer the following questions as best as you can. You have access to the following tools:\",\n",
    "            \"tools\": tools_json\n",
    "        })\n",
    "        query = f\"\"\"{prompt.split(\"Human: \")[-1].strip()}\"\"\"\n",
    "        return ans, query\n",
    "\n",
    "    def _extract_observation(self, prompt: str):\n",
    "        return_json = prompt.split(\"Observation: \")[-1].split(\"\\nThought:\")[0]\n",
    "        self.history.append({\n",
    "            \"role\": \"observation\",\n",
    "            \"content\": return_json\n",
    "        })\n",
    "        return\n",
    "\n",
    "    def _extract_tool(self):\n",
    "        if len(self.history[-1][\"metadata\"]) > 0:\n",
    "            metadata = self.history[-1][\"metadata\"]\n",
    "            content = self.history[-1][\"content\"]\n",
    "            if \"tool_call\" in content:\n",
    "                for tool in self.tool_names:\n",
    "                    if tool in metadata:\n",
    "                        input_para = content.split(\"='\")[-1].split(\"'\")[0]\n",
    "                        action_json = {\n",
    "                            \"action\": tool,\n",
    "                            \"action_input\": input_para\n",
    "                        }\n",
    "                        self.has_search = True\n",
    "                        return f\"\"\"\n",
    "Action: \n",
    "```\n",
    "{json.dumps(action_json, ensure_ascii=False)}\n",
    "```\"\"\"\n",
    "        final_answer_json = {\n",
    "            \"action\": \"Final Answer\",\n",
    "            \"action_input\": self.history[-1][\"content\"]\n",
    "        }\n",
    "        self.has_search = False\n",
    "        return f\"\"\"\n",
    "Action: \n",
    "```\n",
    "{json.dumps(final_answer_json, ensure_ascii=False)}\n",
    "```\"\"\"\n",
    "\n",
    "    def _call(self, prompt: str, history: List = [], stop: Optional[List[str]] = [\"<|user|>\"]):\n",
    "        if history is None:\n",
    "            history=[]\n",
    "            \n",
    "        history.append({\"role\": \"user\", \"content\": prompt})\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=\"glm-4\",  # 填写需要调用的模型名称\n",
    "            messages=history,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        result = response.choices[0].message.content\n",
    "        print(result)\n",
    "        #AIMessage(content=result)\n",
    "        return result\n",
    "\n",
    "    \n",
    "llm = ChatGLM4()\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "template = \"\"\"{question}\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "from langchain import LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "question = \"北京和上海两座城市有什么不同？\"\n",
    "\n",
    "llm_chain.run(question)\n",
    "\n",
    "\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(data)\n",
    "\n",
    "\n",
    "# Requires:\n",
    "# pip install langchain docarray tiktoken\n",
    "import sentence_transformers\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# replace to your model path\n",
    "# embedding_path = \"D:\\\\ai\\\\download\\\\text2vec-large-chinese\"\n",
    "embedding_path=\"D:\\\\ai\\\\bge-large-zh-v1.5\"\n",
    "embeddings = HuggingFaceEmbeddings(model_name=embedding_path)\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    ")\n",
    "\n",
    "\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Summarize the main themes in these retrieved docs: {docs}\"\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "chain.invoke(docs)\n",
    "\n",
    "\n",
    "# Raptor\n",
    "from typing import Dict, List, Optional, Tuple\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from sklearn.mixture import GaussianMixture\n",
    " \n",
    "RANDOM_SEED = 224  # Fixed seed for reproducibility\n",
    " \n",
    " \n",
    "def global_cluster_embeddings(\n",
    "        embeddings: np.ndarray,\n",
    "        dim: int,\n",
    "        n_neighbors: Optional[int] = None,\n",
    "        metric: str = \"cosine\",\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    使用 UMAP 对嵌入执行全局降维。\n",
    "    参数：\n",
    "    - embeddings：作为 numpy 数组的输入嵌入。\n",
    "    - dim：缩减空间的目标维度。\n",
    "    - n_neighbors：可选；数字每个点要考虑的邻居数。如果未提供，则默认为嵌入数量的平方根。\n",
    "    - metric：用于 UMAP 的距离度量。\n",
    "    返回：\n",
    "    - 减少到指定维度的嵌入的 numpy 数组。\n",
    "    \"\"\"\n",
    "    if n_neighbors is None:\n",
    "        n_neighbors = int((len(embeddings) - 1) ** 0.5)\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=n_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    " \n",
    " \n",
    "def local_cluster_embeddings(\n",
    "        embeddings: np.ndarray, dim: int, num_neighbors: int = 10, metric: str = \"cosine\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    使用 UMAP 对嵌入执行局部降维，通常在全局聚类之后。\n",
    "    参数：\n",
    "    - embeddings：作为 numpy 数组的输入嵌入。\n",
    "    - dim：缩减空间的目标维度。\n",
    "    - num_neighbors：每个点要考虑的邻居数量。\n",
    "    - metric：用于 UMAP 的距离度量。\n",
    "    返回：\n",
    "    - 减少到指定维度的嵌入的 numpy 数组。\n",
    "    \"\"\"\n",
    "    return umap.UMAP(\n",
    "        n_neighbors=num_neighbors, n_components=dim, metric=metric\n",
    "    ).fit_transform(embeddings)\n",
    " \n",
    " \n",
    "def get_optimal_clusters(\n",
    "        embeddings: np.ndarray, max_clusters: int = 50, random_state: int = RANDOM_SEED\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    使用贝叶斯信息准则 (BIC) 和高斯混合模型确定最佳簇数。\n",
    "    参数：\n",
    "    - embeddings：作为 numpy 数组的输入嵌入。\n",
    "    - max_clusters：要考虑的最大簇数。\n",
    "    - random_state：可重复性的种子。\n",
    "    返回：\n",
    "    - 表示找到的最佳簇数的整数。\n",
    "    \"\"\"\n",
    "    max_clusters = min(max_clusters, len(embeddings))\n",
    "    n_clusters = np.arange(1, max_clusters)\n",
    "    bics = []\n",
    "    for n in n_clusters:\n",
    "        gm = GaussianMixture(n_components=n, random_state=random_state)\n",
    "        gm.fit(embeddings)\n",
    "        bics.append(gm.bic(embeddings))\n",
    "    return n_clusters[np.argmin(bics)]\n",
    " \n",
    " \n",
    "def GMM_cluster(embeddings: np.ndarray, threshold: float, random_state: int = 0):\n",
    "    \"\"\"\n",
    "    使用高斯聚类嵌入基于概率阈值的混合模型 (GMM)。\n",
    "    参数：\n",
    "    - embeddings：作为 numpy 数组的输入嵌入。\n",
    "    - 阈值：将嵌入分配给簇的概率阈值。\n",
    "    - random_state：可重复性的种子。\n",
    "    返回：\n",
    "    - 包含聚类标签和确定的聚类数量的元组。\n",
    "    \"\"\"\n",
    "    n_clusters = get_optimal_clusters(embeddings)\n",
    "    gm = GaussianMixture(n_components=n_clusters, random_state=random_state)\n",
    "    gm.fit(embeddings)\n",
    "    probs = gm.predict_proba(embeddings)\n",
    "    labels = [np.where(prob > threshold)[0] for prob in probs]\n",
    "    return labels, n_clusters\n",
    " \n",
    " \n",
    "def perform_clustering(\n",
    "        embeddings: np.ndarray,\n",
    "        dim: int,\n",
    "        threshold: float,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    通过首先全局降低嵌入的维数，然后使用高斯混合模型进行聚类，最后对嵌入进行聚类在每个全局聚类中执行局部聚类。\n",
    "    参数：\n",
    "    - embeddings：作为 numpy 数组的输入嵌入。\n",
    "    - dim：UMAP 缩减的目标维数。\n",
    "    - Threshold：在 GMM 中将嵌入分配给聚类的概率阈值。\n",
    "    返回：\n",
    "    - numpy 数组列表，其中每个数组包含每个嵌入的簇 ID。\n",
    "    \"\"\"\n",
    "    if len(embeddings) <= dim + 1:\n",
    "        # Avoid clustering when there's insufficient data\n",
    "        return [np.array([0]) for _ in range(len(embeddings))]\n",
    " \n",
    "    # Global dimensionality reduction\n",
    "    reduced_embeddings_global = global_cluster_embeddings(embeddings, dim)\n",
    "    # Global clustering\n",
    "    global_clusters, n_global_clusters = GMM_cluster(\n",
    "        reduced_embeddings_global, threshold\n",
    "    )\n",
    " \n",
    "    all_local_clusters = [np.array([]) for _ in range(len(embeddings))]\n",
    "    total_clusters = 0\n",
    " \n",
    "    # Iterate through each global cluster to perform local clustering\n",
    "    for i in range(n_global_clusters):\n",
    "        # Extract embeddings belonging to the current global cluster\n",
    "        global_cluster_embeddings_ = embeddings[\n",
    "            np.array([i in gc for gc in global_clusters])\n",
    "        ]\n",
    " \n",
    "        if len(global_cluster_embeddings_) == 0:\n",
    "            continue\n",
    "        if len(global_cluster_embeddings_) <= dim + 1:\n",
    "            # Handle small clusters with direct assignment\n",
    "            local_clusters = [np.array([0]) for _ in global_cluster_embeddings_]\n",
    "            n_local_clusters = 1\n",
    "        else:\n",
    "            # Local dimensionality reduction and clustering\n",
    "            reduced_embeddings_local = local_cluster_embeddings(\n",
    "                global_cluster_embeddings_, dim\n",
    "            )\n",
    "            local_clusters, n_local_clusters = GMM_cluster(\n",
    "                reduced_embeddings_local, threshold\n",
    "            )\n",
    " \n",
    "        # Assign local cluster IDs, adjusting for total clusters already processed\n",
    "        for j in range(n_local_clusters):\n",
    "            local_cluster_embeddings_ = global_cluster_embeddings_[\n",
    "                np.array([j in lc for lc in local_clusters])\n",
    "            ]\n",
    "            indices = np.where(\n",
    "                (embeddings == local_cluster_embeddings_[:, None]).all(-1)\n",
    "            )[1]\n",
    "            for idx in indices:\n",
    "                all_local_clusters[idx] = np.append(\n",
    "                    all_local_clusters[idx], j + total_clusters\n",
    "                )\n",
    " \n",
    "        total_clusters += n_local_clusters\n",
    " \n",
    "    return all_local_clusters\n",
    " \n",
    " \n",
    "def embed(texts):\n",
    "    \"\"\"\n",
    "    为文本文档列表生成嵌入。\n",
    "    此函数假设存在 `embd` 对象，其方法 `embed_documents`\n",
    "    接受文本列表并返回其嵌入。\n",
    "    参数：\n",
    "    - texts：List[str]，要嵌入的文本文档列表。\n",
    "    返回：\n",
    "    - numpy.ndarray：嵌入数组\n",
    "    \"\"\"\n",
    "    text_embeddings = embd.embed_documents(texts)\n",
    "    text_embeddings_np = np.array(text_embeddings)\n",
    "    return text_embeddings_np\n",
    " \n",
    " \n",
    "def embed_cluster_texts(texts):\n",
    "    \"\"\"\n",
    "    嵌入文本列表并对它们进行聚类，返回包含文本的DataFrame 、它们的嵌入和聚类标签。\n",
    "    该函数将嵌入生成和聚类合并为一个步骤。它假设存在先前定义的“perform_clustering”函数，该函数对嵌入执行聚类。\n",
    "    参数：\n",
    "        - texts：List[str]，要处理的文本文档列表。\n",
    "    返回：\n",
    "        - pandas.DataFrame：包含原始文本、其嵌入以及分配的簇标签的 DataFrame。\n",
    "    \"\"\"\n",
    "    text_embeddings_np = embed(texts)  # Generate embeddings\n",
    "    cluster_labels = perform_clustering(\n",
    "        text_embeddings_np, 10, 0.1\n",
    "    )  # Perform clustering on the embeddings\n",
    "    df = pd.DataFrame()  # Initialize a DataFrame to store the results\n",
    "    df[\"text\"] = texts  # Store original texts\n",
    "    df[\"embd\"] = list(text_embeddings_np)  # Store embeddings as a list in the DataFrame\n",
    "    df[\"cluster\"] = cluster_labels  # Store cluster labels\n",
    "    return df\n",
    " \n",
    " \n",
    "def fmt_txt(df: pd.DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    将 DataFrame 中的文本文档格式化为单个字符串.\n",
    "    参数：\n",
    "    - df：包含要格式化的文本文档的 'text' 列的 DataFrame。\n",
    "    返回：\n",
    "    - 所有文本文档均由特定分隔符连接的单个字符串。\n",
    "    \"\"\"\n",
    "    unique_txt = df[\"text\"].tolist()\n",
    "    return \"--- --- \\n --- --- \".join(unique_txt)\n",
    " \n",
    " \n",
    "def embed_cluster_summarize_texts(\n",
    "        texts: List[str], level: int\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    嵌入、聚类和总结文本列表。此函数首先生成文本的嵌入，\n",
    "    根据相似性对它们进行聚类，扩展聚类分配以便于处理，然后总结每个聚类内的内容。\n",
    "    参数：\n",
    "    - texts：要处理的文本文档列表。\n",
    "    - level：一个整数参数，可以定义处理的深度或细节。\n",
    "    返回：\n",
    "    - 包含两个 DataFrame 的元组：\n",
    "      1. 第一个 DataFrame (`df_clusters`) 包含原始文本、它们的嵌入和聚类分配。\n",
    "      2. 第二个 DataFrame (`df_summary`) 包含每个集群的摘要、指定的详细级别以及集群标识符。\n",
    "    \"\"\"\n",
    " \n",
    "    # Embed and cluster the texts, resulting in a DataFrame with 'text', 'embd', and 'cluster' columns\n",
    "    df_clusters = embed_cluster_texts(texts)\n",
    " \n",
    "    # Prepare to expand the DataFrame for easier manipulation of clusters\n",
    "    expanded_list = []\n",
    " \n",
    "    # Expand DataFrame entries to document-cluster pairings for straightforward processing\n",
    "    for index, row in df_clusters.iterrows():\n",
    "        for cluster in row[\"cluster\"]:\n",
    "            expanded_list.append(\n",
    "                {\"text\": row[\"text\"], \"embd\": row[\"embd\"], \"cluster\": cluster}\n",
    "            )\n",
    " \n",
    "    # Create a new DataFrame from the expanded list\n",
    "    expanded_df = pd.DataFrame(expanded_list)\n",
    " \n",
    "    # Retrieve unique cluster identifiers for processing\n",
    "    all_clusters = expanded_df[\"cluster\"].unique()\n",
    " \n",
    "    print(f\"--Generated {len(all_clusters)} clusters--\")\n",
    " \n",
    "    # Summarization\n",
    "    template = \"\"\"Here is a sub-set of LangChain Expression Langauge doc.\n",
    "    LangChain Expression Langauge provides a way to compose chain in LangChain.\n",
    "    Give a detailed summary of the documentation provided.\n",
    "    Documentation:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    chain = prompt | model | StrOutputParser()\n",
    " \n",
    "    # Format text within each cluster for summarization\n",
    "    summaries = []\n",
    "    for i in all_clusters:\n",
    "        df_cluster = expanded_df[expanded_df[\"cluster\"] == i]\n",
    "        formatted_txt = fmt_txt(df_cluster)\n",
    "        summaries.append(chain.invoke({\"context\": formatted_txt}))\n",
    " \n",
    "    # Create a DataFrame to store summaries with their corresponding cluster and level\n",
    "    df_summary = pd.DataFrame(\n",
    "        {\n",
    "            \"summaries\": summaries,\n",
    "            \"level\": [level] * len(summaries),\n",
    "            \"cluster\": list(all_clusters),\n",
    "        }\n",
    "    )\n",
    " \n",
    "    return df_clusters, df_summary\n",
    " \n",
    " \n",
    "def recursive_embed_cluster_summarize(\n",
    "        texts: List[str], level: int = 1, n_levels: int = 3\n",
    ") -> Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    递归地嵌入、聚类和汇总文本，直到指定级别或直到唯一聚类的数量变为 1，存储每个级别的结果。\n",
    "    参数：\n",
    "    - texts：List[str]，要处理的文本。\n",
    "    - level：int，当前递归级别（从1开始）。\n",
    "    - n_levels：int，最大递归深度。\n",
    "    返回：\n",
    "    - Dict[int, Tuple[pd.DataFrame, pd.DataFrame]]，一个字典，其中键是递归级别，值是包含该级别的簇 DataFrame 和摘要 DataFrame 的元组。\n",
    "    \"\"\"\n",
    "    results = {}  # Dictionary to store results at each level\n",
    " \n",
    "    # Perform embedding, clustering, and summarization for the current level\n",
    "    df_clusters, df_summary = embed_cluster_summarize_texts(texts, level)\n",
    " \n",
    "    # Store the results of the current level\n",
    "    results[level] = (df_clusters, df_summary)\n",
    " \n",
    "    # Determine if further recursion is possible and meaningful\n",
    "    unique_clusters = df_summary[\"cluster\"].nunique()\n",
    "    if level < n_levels and unique_clusters > 1:\n",
    "        # Use summaries as the input texts for the next level of recursion\n",
    "        new_texts = df_summary[\"summaries\"].tolist()\n",
    "        next_level_results = recursive_embed_cluster_summarize(\n",
    "            new_texts, level + 1, n_levels\n",
    "        )\n",
    " \n",
    "        # Merge the results from the next level into the current results dictionary\n",
    "        results.update(next_level_results)\n",
    " \n",
    "    return results\n",
    "\n",
    "\n",
    "leaf_texts = docs\n",
    "results = recursive_embed_cluster_summarize(leaf_texts, level=1, n_levels=3)\n",
    "\n",
    "# Initialize all_texts with leaf_texts\n",
    "all_texts = leaf_texts.copy()\n",
    " \n",
    "# Iterate through the results to extract summaries from each level and add them to all_texts\n",
    "for level in sorted(results.keys()):\n",
    "    # Extract summaries from the current level's DataFrame\n",
    "    summaries = results[level][1][\"summaries\"].tolist()\n",
    "    # Extend all_texts with the summaries from the current level\n",
    "    all_texts.extend(summaries)\n",
    "#Final Summaries extracted\n",
    "print(all_texts)\n",
    "\n",
    "vectorstore = Chroma.from_texts(texts=all_texts, embedding=embddinga) \n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    " \n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    " \n",
    " \n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    " \n",
    " \n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(prompt)\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
